# CS 285 final project

## Project specification

The final project in this course requires implementing, evaluating, and documenting a new research idea in the field of deep reinforcement learning. Students will be expected to prepare a proposal, milestone report, and final report. Students will also provide peer feedback on other groupsâ€™ proposals and milestones.

### Deadlines

* September 21: proposal
* October 5: proposal peer feedback
* October 26: milestone
* November 9: milestone peer feedback
* December 14: final project report

### Project proposal

* $\leq$ one page
* describe the idea
* answer the following questions:

1. Which tasks or problems will you study? Where will you get your data or simulator (or real-world system)?
2. What is the main research hypothesis your project will be investigate? All projects should at least attempt to evaluate novel ideas that pertain to deep RL or its applications.
3. How does the topic of your project relate to deep RL?

### Milestone Report

* $\leq$ one page
* answer the following questions:

1. What experiments have you conducted so far?
2. Are there any changes to the research hypothesis or problem statement from the proposal?

* provide details of at least one experiment conducted since the proposal. This experiment does not need to be successful, but you should have attempted something.
* If it did not work as
expected, you should briefly discuss why.
* You are encouraged to attach a plot or figure.

Like the proposal,
the milestone report will be graded by course staff, and you will also receive feedback from your peers in a
peer review process. The reviews will not impact your grade, but are entirely there to provide more feedback
for you.

### Final report

In the style of a research paper, with an abstract and a main body.

Abstract:

* one-page
* main findings and accomplishments
* may be submitted as a separate pdf file, or attached as the first page of the full report.

Main body:

* $\approx$ 8 pages
* describe and motivate the method in detail
* results
* figures
* a few sentences on what each member of the group contributed

## Authors

Yuxi Liu

Ramanan Abeyakaran

## Some papers

Schmidhuber: have been working on this since the 1990s. His approach stems from algorithmic information theory (basically, enumerate all possible programs, with probability proportional to 2^{-n}, where n is the least number of bits needed to define the program).
[Frontiers | PowerPlay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00313/full)

Barto: The Barto as in "Sutton and Barto". Have been working on this also for decades. His approach is more in the tradition of psychology and biology. Hints can be seen in the Sutton and Barto book, which has a whole chapter on psychology.
[Intrinsic Motivation and Reinforcement Learning | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-642-32375-1_2)

Botvinick: a very fun neuroscientist-AI scientist. 
[Reinforcement learning: Fast and slow - Matthew Botvinick - YouTube](https://www.youtube.com/watch?v=b0LddBiF5jM)
[Reinforcement Learning, Fast and Slow - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1364661319300610#fig0005)

More references in the bibliography of this paper.
[Frontiers | Intrinsic motivations and open-ended development in animals, humans, and robots: an overview](https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00985/full)